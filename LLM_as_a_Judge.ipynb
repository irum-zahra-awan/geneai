{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eF3GT_XRPeP"
      },
      "source": [
        "# Build and Evaluate RAG with Gemini using LLM as a JUDGE\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/irum-zahra-awan/geneai/blob/main/podcast_agent.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.comirum-zahra-awan/geneai/blob/main/podcast_agent.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB9hq6aXRhuD"
      },
      "source": [
        "| Author |\n",
        "| --- |\n",
        "| [Irum Zahra](https://github.com/irum-zahra-awan/) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzZhzc-TrEKu"
      },
      "source": [
        "# **Part 1: Downloading and Processing Papers with Python**\n",
        "\n",
        "To use the information within these papers, we first need to get their content into our Python environment. Since these papers are available as web pages or PDFs, we will write a script to:\n",
        "\n",
        "\n",
        "* **Fetch the content:** We'll use the requests library to download the HTML of the web pages. For the PDF, we'll use a library designed to extract text from PDF files.\n",
        "\n",
        "* **Parse the text:** Raw HTML contains a lot of code we don't need. We'll use BeautifulSoup to parse the HTML and extract only the meaningful text. For PDFs, pypdf will help us extract text directly.\n",
        "\n",
        "* **Chunk the text:** Large language models have a limited context window (the amount of text they can consider at once). A single research paper is far too long. To handle this, we break the text into smaller, overlapping \"***chunks.***\" This ensures that the model receives manageable pieces of information and that semantic meaning isn't lost at the boundaries of chunks.\n",
        "We use the `RecursiveCharacterTextSplitter` from langchain for this, a standard tool for this task.\n",
        "\n",
        "This entire process prepares the raw data for the next crucial step: creating vector embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wckH_36arA9V",
        "outputId": "ffd674a4-10d8-445f-a04f-701d74ce5846"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.5/305.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.0/101.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install requests beautifulsoup4 pypdf langchain -q -U\n",
        "%pip install google-cloud-aiplatform langchain-google-vertexai faiss-cpu langchain-community -q -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7ToclvX0sCff"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from pypdf import PdfReader\n",
        "\n",
        "\n",
        "import sys\n",
        "from google.colab import auth\n",
        "import vertexai\n",
        "\n",
        "from langchain_google_vertexai import VertexAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pHVRcFKesKgT"
      },
      "outputs": [],
      "source": [
        "# Define URLs and a directory to save the text\n",
        "urls = [\n",
        "    \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11680066/\",\n",
        "    \"https://www.downtoearth.org.in/health/world-polio-day-2024-conflict-delays-and-vaccine-shortages-derail-global-eradication-efforts\",\n",
        "]\n",
        "\n",
        "# The CDC paper is a PDF, so we handle it separately\n",
        "pdf_url = \"https://www.cdc.gov/mmwr/volumes/73/wr/pdfs/mm7341a1-H.pdf\"\n",
        "pdf_filename = \"cdc_polio_report_2024.pdf\"\n",
        "\n",
        "# Create a directory to store the processed text\n",
        "if not os.path.exists(\"polio_papers\"):\n",
        "    os.makedirs(\"polio_papers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Sh-293JsoeV",
        "outputId": "bdb2fae6-2258-46fc-ee7d-b1da8e02dfb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error downloading paper 0: 403 Client Error: Forbidden for url: https://pmc.ncbi.nlm.nih.gov/articles/PMC11680066/\n",
            "Could not find main content for paper 1\n",
            "Successfully downloaded cdc_polio_report_2024.pdf\n",
            "Successfully extracted and saved text from cdc_polio_report_2024.pdf\n"
          ]
        }
      ],
      "source": [
        "# Function to download and parse HTML content\n",
        "\n",
        "def scrape_and_save_text(url, index):\n",
        "    \"\"\"Scrapes text from a URL and saves it to a file.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Find the main content of the article (selectors might need adjustment)\n",
        "        # For ncbi.nlm.nih.gov\n",
        "        if \"ncbi.nlm.nih.gov\" in url:\n",
        "            content = soup.find('div', id='__article')\n",
        "        # For downtoearth.org.in\n",
        "        elif \"downtoearth.org.in\" in url:\n",
        "            content = soup.find('div', class_='news-detail-content')\n",
        "        else:\n",
        "            content = soup.body # Fallback to the whole body\n",
        "\n",
        "        if content:\n",
        "            text = content.get_text(separator='\\n', strip=True)\n",
        "            filename = f\"polio_papers/paper_{index}.txt\"\n",
        "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(text)\n",
        "            print(f\"Successfully scraped and saved paper {index}\")\n",
        "            return text\n",
        "        else:\n",
        "            print(f\"Could not find main content for paper {index}\")\n",
        "            return \"\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading paper {index}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Function to download and extract text from a PDF\n",
        "def download_and_extract_pdf_text(url, filename):\n",
        "    \"\"\"Downloads a PDF and extracts its text content.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        with open(filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"Successfully downloaded {filename}\")\n",
        "\n",
        "        # Extract text from the downloaded PDF\n",
        "        reader = PdfReader(filename)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() or \"\"\n",
        "\n",
        "        # Save the extracted text\n",
        "        text_filename = \"polio_papers/paper_2.txt\"\n",
        "        with open(text_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(text)\n",
        "        print(f\"Successfully extracted and saved text from {filename}\")\n",
        "        return text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading PDF: {e}\")\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "# Execute the functions and process all papers\n",
        "all_texts = []\n",
        "# Scrape HTML papers\n",
        "for i, url in enumerate(urls):\n",
        "    all_texts.append(scrape_and_save_text(url, i))\n",
        "\n",
        "# Download and process the PDF paper\n",
        "all_texts.append(download_and_extract_pdf_text(pdf_url, pdf_filename))\n",
        "\n",
        "# Combine all text into a single document for chunking\n",
        "full_text = \"\\n\\n--- NEW PAPER ---\\n\\n\".join(filter(None, all_texts))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_ET9iutuP0b"
      },
      "source": [
        "# **Part 2: Storing Information in a Vector Database**\n",
        "\n",
        "A vector database allows us to perform semantic search. Instead of just searching for keywords, we can search for concepts and meanings. Here's how it works:\n",
        "\n",
        "* **Embedding Model:** We use a powerful model from Vertex AI (like textembedding-gecko) to convert our text chunks into numerical representations called vectors or embeddings. Each vector is a list of numbers that captures the semantic meaning of the text. Chunks with similar meanings will have vectors that are \"close\" to each other in mathematical space.\n",
        "\n",
        "* **Vector Store:** We need a place to store these vectors and a way to search through them efficiently. FAISS (Facebook AI Similarity Search) is a lightweight and highly efficient library for this purpose. It's perfect for a Colab environment as it runs in memory and doesn't require a separate database server.\n",
        "\n",
        "* **Storing:** The script will take each text chunk, pass it to the Vertex AI embedding model to get a vector, and then store that vector (along with the original text chunk) in our FAISS index.\n",
        "\n",
        "\n",
        "This setup is the core of our RAG system. It allows us to take a user's question, find the most relevant chunks of information from our research papers, and use them to generate a factual, context-aware answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RU3TLO1Jso5D",
        "outputId": "bf0d766f-edd8-4000-a7bf-86350ccfc2ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Total number of text chunks: 29\n",
            "Sample chunk:\n",
            "Morbidity and Mortality Weekly Report\n",
            "U.S. Centers for Disease Control and Prevention\n",
            "Weekly / Vol. 73 / No. 41 O ctober 17, 2024\n",
            "INSIDE\n",
            "917 T obacco Product Use Among Middle and High \n",
            "School Students — National Youth Tobacco Survey, \n",
            "United States, 2024\n",
            "925 C overage with Selected Vaccines and Exemption \n",
            "Rates Among Children in Kindergarten — United \n",
            "States, 2023–24 School Year\n",
            "933 Not es from the Field: Enhanced Surveillance for \n",
            "Raccoon Rabies Virus Variant and Vaccination of \n",
            "Wildlife for Management — Omaha, Nebraska, \n",
            "October 2023–July 2024 \n",
            "936 QuickStats\n",
            "Continuing Education examination available at  \n",
            "https://www.cdc.gov/mmwr/mmwr_continuingEducation.html\n",
            "Update on Vaccine-Derived Poliovirus Outbreaks —  \n",
            "Worldwide, January 2023–June 2024\n",
            "Apophia Namageyo-Funa, PhD1; Sharon A. Greene, PhD1; Elizabeth Henderson2; Mohamed A. T raoré3; Shahzad Shaukat, PhD3; John Paul Bigouette, PhD1; \n",
            "Jaume Jorba, PhD2; Eric Wiesen, DrPH1; Omotayo Bolu, PhD1; Ousmane M. Diop, PhD3; Cara C. Burns, PhD2; Steven G.F . Wassilak, MD1\n",
            "Abstract\n",
            "Circulating vaccine-derived polioviruses (cVDPVs) can \n",
            "emerge and lead to outbreaks of paralytic polio as well as \n",
            "asymptomatic transmission in communities with a high percent-\n",
            "age of undervaccinated children. Using data from the World \n",
            "Health Organization Polio Information System and Global \n",
            "Polio Laboratory Network, this report describes global polio \n",
            "outbreaks due to cVDPVs during January 2023–June 2024\n"
          ]
        }
      ],
      "source": [
        "# Chunk the text\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1500,  # The size of each chunk in characters\n",
        "    chunk_overlap=200 # Number of characters to overlap between chunks\n",
        ")\n",
        "chunks = text_splitter.split_text(full_text)\n",
        "\n",
        "print(f\"\\nTotal number of text chunks: {len(chunks)}\")\n",
        "print(\"Sample chunk:\")\n",
        "print(chunks[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XNlbrcsEssTR"
      },
      "outputs": [],
      "source": [
        "# Authenticate user\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qj86yI5yswGY"
      },
      "outputs": [],
      "source": [
        "# Authenticate and initialize Vertex AI\n",
        "\n",
        "# Define your Google Cloud project\n",
        "PROJECT_ID = \"your-project-id\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\" # @param {type:\"string\"}\n",
        "\n",
        "# Initialize Vertex AI\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTqu34K4s92z",
        "outputId": "5c3f8f12-0867-4e06-cbdf-9dcfdef50a13"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/vertexai/_model_garden/_model_garden_models.py:278: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating vector store... This may take a few minutes.\n",
            "Vector store created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Set up the embedding model and FAISS vector store\n",
        "# Initialize the embedding model\n",
        "embeddings = VertexAIEmbeddings(model_name=\"text-embedding-005\")\n",
        "\n",
        "# Create the vector store from our text chunks\n",
        "# This will take a moment as it processes each chunk and gets its embedding\n",
        "print(\"Creating vector store... This may take a few minutes.\")\n",
        "vector_store = FAISS.from_texts(chunks, embeddings)\n",
        "print(\"Vector store created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgvlCw2Os9q2",
        "outputId": "6ad353cd-97d5-4b10-c799-3896e7d21fe4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Sample Retrieval for query: 'What are the main challenges in polio eradication?' ---\n",
            "\n",
            "--- Relevant Chunk 1 ---\n",
            "To achieve the Global Polio Eradication Initiative’s goal of interrupt -\n",
            "ing cVDPV transmission by 2026, outbreak responses must be \n",
            "timely and overcome barriers to reaching children who are missed \n",
            "by routine and supplementary immunization activities.\n",
            "Limitations\n",
            "The findings in this report are subject to at least two limita-\n",
            "tions. First, existing gaps in polio surveillance systems might \n",
            "lead to the underestimation of cases and transmission levels \n",
            "and inaccuracies in the geographic spread of cVDPVs. Second, \n",
            "delays in the transportation of polio samples and testing by \n",
            "reference laboratories might result in underreporting of cases, \n",
            "outbreaks, and emergences during January–June 2024.\n",
            "Implications for Public Health Practice\n",
            "GPEI currently aims to eradicate polio by 2026; the key \n",
            "challenges are ending transmission in security-compromised \n",
            "areas and hard-to-reach communities and preventing any fur-\n",
            "ther international spread. Ending transmission by 2026 will \n",
            "require a focus on implementing intensive efforts to vaccinate \n",
            "children in security-compromised and hard-to-reach commu-\n",
            "nities to achieve the goal of sustained cVDPV2 interruption. \n",
            "Countries can control cVDPV outbreaks with timely alloca -\n",
            "tion of resources to implement prompt, high-quality responses \n",
            "after outbreak confirmation. Stopping all cVDPV transmis -\n",
            "sion requires effectively increasing population immunity by \n",
            "overcoming the barriers to reaching children.\n",
            "Acknowledgments\n",
            "--------------------\n",
            "\n",
            "--- Relevant Chunk 2 ---\n",
            "after outbreak confirmation. Stopping all cVDPV transmis -\n",
            "sion requires effectively increasing population immunity by \n",
            "overcoming the barriers to reaching children.\n",
            "Acknowledgments\n",
            "World Health Organization (WHO) Global Polio Laboratory \n",
            "Network (GPLN) laboratories; GPLN regional laboratory \n",
            "coordinators; field surveillance officers at the WHO Eastern \n",
            "Mediterranean Regional Office, WHO Regional Office for the \n",
            "Americas, WHO European Regional Office, WHO Western Pacific \n",
            "Regional Office, WHO South-East Asian Regional Office, WHO \n",
            "African Regional Office; staff members of the Polio Eradication \n",
            "Branch, Global Immunization Division, Center for Global Health, \n",
            "CDC; staff members of the Polio and Picornavirus Branch, Division \n",
            "of Viral Diseases, National Center for Immunization and Respiratory \n",
            "Diseases, CDC; Geospatial Research, Analysis, and Services Program, \n",
            "Agency for T oxic Substances and Disease Registry, CDC; Emergency \n",
            "Operations Center, Center for Preparedness and Response, CDC.\n",
            "Corresponding author: Apophia Namageyo-Funa, aen5@cdc.gov.\n",
            " 1Global Immunization Division, Center for Global Health, CDC; 2Division of \n",
            "Viral Diseases, National Center for Immunization and Respiratory Diseases, CDC; \n",
            "3Polio Eradication Department, World Health Organization, Geneva, Switzerland.\n",
            "All authors have completed and submitted the International \n",
            "Committee of Medical Journal Editors form for disclosure of potential\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "# Test the vector store with a sample query\n",
        "# Let's see what information it retrieves for a sample question\n",
        "sample_query = \"What are the main challenges in polio eradication?\"\n",
        "retrieved_docs = vector_store.similarity_search(sample_query, k=2) # Get the top 2 most relevant chunks\n",
        "\n",
        "print(f\"\\n--- Sample Retrieval for query: '{sample_query}' ---\")\n",
        "for i, doc in enumerate(retrieved_docs):\n",
        "    print(f\"\\n--- Relevant Chunk {i+1} ---\")\n",
        "    print(doc.page_content)\n",
        "    print(\"--------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_owpzmnYvNfm"
      },
      "source": [
        "# **Part 4: Generating Questions for the LLM**\n",
        "Based on the content of the three papers, here are five insightful questions that require the LLM to synthesize information from multiple sources.\n",
        "\n",
        "* Question 1: According to the provided documents, what are the primary reasons for the persistence of circulating vaccine-derived poliovirus (cVDPV) outbreaks in 2023-2024, and which type is most prevalent?\n",
        "\n",
        "* Question 2: The papers mention both Wild Poliovirus (WPV1) and vaccine-derived polioviruses (cVDPV). Compare the geographical regions primarily affected by each in 2024.\n",
        "\n",
        "* Question 3: What specific challenges, such as conflict and vaccine supply, have hindered polio vaccination campaigns in 2024, and what are the recommended strategies to overcome them?\n",
        "\n",
        "* Question 4: How has the COVID-19 pandemic impacted global polio vaccination coverage, and what are the long-term risks associated with these disruptions?\n",
        "\n",
        "* Question 5: Explain the role of the novel oral polio vaccine type 2 (nOPV2). What are its benefits, and what issues have been reported regarding its supply and deployment?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJiN91QAzfVa"
      },
      "source": [
        "# **Part 5: Answering Queries with a RAG System**\n",
        "\n",
        "This is where everything comes together. We will now build the complete RAG pipeline to answer our generated questions.\n",
        "\n",
        "* **Retriever:** The vector store we built (FAISS) acts as our retriever. When a user asks a question, the retriever's job is to quickly find and \"retrieve\" the most relevant text chunks from our source documents.\n",
        "\n",
        "* **Prompt Template:** We don't just send the user's question to the LLM. We create a structured prompt. This prompt instructs the LLM on how to behave (e.g., \"be a helpful assistant\"), provides the retrieved text chunks as context, and then presents the user's question. This guides the model to base its answer only on the information we've provided.\n",
        "\n",
        "* **LLM:** We use a powerful Gemini model from Vertex AI as the \"brain\" of our operation. It will receive the formatted prompt (with context) and generate a coherent, human-like answer.\n",
        "\n",
        "* **Chain**: We use langchain to tie these components together into a RetrievalQA chain. This chain automates the entire process: a question goes in, and a fully formed, context-aware answer comes out.\n",
        "\n",
        "This RAG approach is far superior to simply asking the LLM a question directly because it grounds the model's response in our specific source material, dramatically reducing the risk of hallucinations (made-up information) and ensuring the answers are factual and relevant to our documents.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dIFvcdFi91e7"
      },
      "outputs": [],
      "source": [
        "# Set up the LLM and the QA Chain\n",
        "from langchain_google_vertexai import VertexAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Initialize the Gemini LLM\n",
        "llm = VertexAI(model_name=\"gemini-2.0-flash-001\", temperature=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YfogGfbZ93tI"
      },
      "outputs": [],
      "source": [
        "# Create a prompt template\n",
        "prompt_template = \"\"\"\n",
        "You are a helpful assistant specialized in summarizing information from medical research papers.\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer from the context, just say that you don't know, don't try to make up an answer.\n",
        "Be concise and provide the answer based only on the provided text.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1vjxcBih9-ff"
      },
      "outputs": [],
      "source": [
        "# Create the RetrievalQA Chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vector_store.as_retriever(),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": PROMPT}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwk0MvajuB6l",
        "outputId": "1f12c688-94aa-4c00-bb3a-44b1a41fb7c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Asking question: According to the provided documents, what are the primary reasons for the persistence of circulating vaccine-derived poliovirus (cVDPV) outbreaks in 2023-2024, and which type is most prevalent?\n",
            "\n",
            "--- Generated Answer ---\n",
            "The primary reasons for the persistence of cVDPV outbreaks are delayed implementation of outbreak response campaigns, low-quality campaigns, and barriers to reaching children. cVDPV type 2 is the most prevalent.\n",
            "\n",
            "\n",
            "--- Source Documents Used ---\n",
            "\n",
            "- Source: MADAGASCAR\n",
            "INDONESIA\n",
            "UGANDA\n",
            "BU\n",
            "Country or area with at \n",
            "least one detection of \n",
            "cVDPV through \n",
            "environmental surveillance\n",
            "No environmental detections\n",
            "Not applicable\n",
            "cVDPV type 1\n",
            "cVDPV type 2\n",
            "Acute /f_...\n",
            "\n",
            "- Source: by the end of 2026. Continued circulation of cVDPVs high-\n",
            "lights the need for 1) increased urgency to implement prompt, \n",
            "high-quality SIAs upon detection of new cVDPV outbreaks and \n",
            "2) enhanced effort...\n",
            "\n",
            "- Source: same dates, symbols will overlap; thus, not all isolates are visible. Data as of September 18, 2024, for all emergences.\n",
            "mobilized within countries, to implement intensive response \n",
            "efforts with cross...\n",
            "\n",
            "- Source: Health Organization Polio Information System and Global \n",
            "Polio Laboratory Network, this report describes global polio \n",
            "outbreaks due to cVDPVs during January 2023–June 2024 \n",
            "and updates previous repor...\n"
          ]
        }
      ],
      "source": [
        "# Ask one of our generated questions\n",
        "question_to_ask = \"According to the provided documents, what are the primary reasons for the persistence of circulating vaccine-derived poliovirus (cVDPV) outbreaks in 2023-2024, and which type is most prevalent?\"\n",
        "\n",
        "print(f\"Asking question: {question_to_ask}\")\n",
        "result = qa_chain({\"query\": question_to_ask})\n",
        "\n",
        "# Print the results\n",
        "print(\"\\n--- Generated Answer ---\")\n",
        "print(result[\"result\"])\n",
        "\n",
        "print(\"\\n--- Source Documents Used ---\")\n",
        "for doc in result[\"source_documents\"]:\n",
        "    print(f\"\\n- Source: {doc.page_content[:200]}...\") # Print snippet of the source"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXFcEpbWDsA8"
      },
      "source": [
        "# **Part 6: Using an LLM as a Judge**\n",
        "\n",
        "How do we know if our RAG system is providing good answers? We can evaluate them manually, but this is time-consuming. An advanced and powerful technique is to use another **LLM as an impartial \"judge.\"**\n",
        "\n",
        "* **The Judge's Task:** We give the judge LLM a very specific set of instructions. Its job is not to answer the original question, but to evaluate the answer generated by our RAG system.\n",
        "\n",
        "* **Evaluation Criteria:** We define clear criteria for the judge. In this script, we ask it to assess two key aspects:\n",
        "\n",
        " * **Faithfulness:** Is the generated answer fully supported by the provided source documents? It should not contain information that isn't in the context.\n",
        "\n",
        " * **Relevance:** Does the answer directly address the user's question?\n",
        "\n",
        "* **Structured Output:** We instruct the judge to provide its reasoning and a final verdict (\"SUPPORTED\" or \"NOT SUPPORTED\") in a structured format. This makes the evaluation easy to interpret.\n",
        "\n",
        "Using an LLM Judge automates the evaluation process, allowing us to quickly assess the quality of our RAG system's responses. It's a key part of building robust and reliable AI systems.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "g1OTDrrOFdlf"
      },
      "outputs": [],
      "source": [
        "# Set up the Judge LLM and Prompt Template\n",
        "judge_llm = VertexAI(model_name=\"gemini-2.5-pro\", temperature=0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "VsKBHfB9Fj6g",
        "outputId": "b76a28c9-fd7c-4a08-e178-cb61fabab859"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'There are **eight** planets in our solar system.\\n\\nIn order from the Sun, they are:\\n1.  **Mercury**\\n2.  **Venus**\\n3.  **Earth**\\n4.  **Mars**\\n5.  **Jupiter**\\n6.  **Saturn**\\n7.  **Uranus**\\n8.  **Neptune**\\n\\n### What Happened to Pluto?\\n\\nYou might remember learning that there were nine planets. For a long time, Pluto was considered the ninth planet. However, in 2006, the International Astronomical Union (IAU) established a new definition for a planet.\\n\\nAccording to the IAU, a celestial body must meet three criteria to be classified as a planet:\\n1.  It must orbit the Sun.\\n2.  It must have enough mass to be pulled into a nearly round shape by its own gravity (hydrostatic equilibrium).\\n3.  It must have \"cleared the neighborhood\" around its orbit, meaning it is the dominant gravitational body in its orbital path.\\n\\nPluto meets the first two criteria, but it fails the third. Its orbit is located in the Kuiper Belt, a region full of other icy objects, and it has not cleared this neighborhood.\\n\\nBecause of this, Pluto was reclassified as a **dwarf planet**, a category that also includes other bodies like Eris, Ceres, Makemake, and Haumea.\\n\\nSo, the official count is **eight planets**.'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"How many planets exist in the solar system?\"\n",
        "judge_llm(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "FdLKqwyUDrtr"
      },
      "outputs": [],
      "source": [
        "judge_prompt_template = \"\"\"\n",
        "You are an expert evaluator. Your task is to determine if a generated answer is faithful to the provided source documents.\n",
        "\n",
        "You will be given:\n",
        "1. The original question.\n",
        "2. The source documents (context) used to generate the answer.\n",
        "3. The generated answer.\n",
        "\n",
        "Your evaluation criteria are:\n",
        "- **Faithfulness**: The answer must be fully supported by the information in the source documents. It should not add any new information or contradict the source.\n",
        "- **Relevance**: The answer must be relevant to the original question.\n",
        "\n",
        "Provide a step-by-step reasoning for your decision and then conclude with a final verdict in the format: \"Final Verdict: [SUPPORTED or NOT SUPPORTED]\".\n",
        "\n",
        "---\n",
        "Original Question:\n",
        "{question}\n",
        "\n",
        "---\n",
        "Source Documents:\n",
        "{context}\n",
        "\n",
        "---\n",
        "Generated Answer:\n",
        "{answer}\n",
        "---\n",
        "\n",
        "Reasoning:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "dbVfEmYX_B7c"
      },
      "outputs": [],
      "source": [
        "# Prepare the inputs for the judge\n",
        "# We will use the results from the previous step\n",
        "question = result[\"query\"]\n",
        "context_docs = \"\\n\\n\".join([doc.page_content for doc in result[\"source_documents\"]])\n",
        "generated_answer = result[\"result\"]\n",
        "\n",
        "# Format the input for the judge\n",
        "judge_input = judge_prompt_template.format(\n",
        "    question=question,\n",
        "    context=context_docs,\n",
        "    answer=generated_answer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1STxvZJa-1nT",
        "outputId": "4525b52e-7c8b-4f9d-8188-6391b41d8c71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- ⚖️ Submitting to LLM Judge for Evaluation ⚖️ ---\n",
            "\n",
            "--- Judge's Evaluation ---\n",
            "**Reasoning:**\n",
            "\n",
            "1.  **Analyze the first part of the answer:** \"The primary reasons for the persistence of cVDPV outbreaks are delayed implementation of outbreak response campaigns, low-quality campaigns, and barriers to reaching children.\"\n",
            "    *   The source document states: \"Delayed implementation of outbreak response campaigns and low-quality campaigns have resulted in further international spread.\" This directly supports the first two reasons.\n",
            "    *   The source document also states: \"Stopping all cVDPV transmission requires effectively increasing population immunity by overcoming barriers to reaching children.\" This directly supports the third reason.\n",
            "    *   Another sentence reinforces this: \"Continued circulation of cVDPVs highlights the need for... enhanced efforts... to vaccinate children in security-compromised areas and in hard-to-reach communities.\"\n",
            "\n",
            "2.  **Analyze the second part of the answer:** \"cVDPV type 2 is the most prevalent.\"\n",
            "    *   The source document states: \"Three countries reported cVDPV type 1 (cVDPV1) outbreaks and 38 countries reported cVDPV type 2 (cVDPV2) outbreaks; two of these countries reported cocirculating cVDPV1 and cVDPV2.\" The comparison of 3 countries for type 1 versus 38 countries for type 2 clearly supports the claim that type 2 is the most prevalent.\n",
            "\n",
            "3.  **Conclusion:** Both claims in the generated answer are directly and explicitly supported by information found in the source documents. The answer is faithful and relevant.\n",
            "\n",
            "**Final Verdict: [SUPPORTED]**\n"
          ]
        }
      ],
      "source": [
        "# Get the evaluation from the judge\n",
        "print(\"\\n--- ⚖️ Submitting to LLM Judge for Evaluation ⚖️ ---\")\n",
        "evaluation = judge_llm(judge_input)\n",
        "\n",
        "# Print the judge's verdict\n",
        "print(\"\\n--- Judge's Evaluation ---\")\n",
        "print(evaluation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EB73mP7cM6CQ",
        "outputId": "c33f0bc9-002c-4246-d26e-9f66cd6e444e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- ⚖️ Submitting MODIFIED Answer to LLM Judge ⚖️ ---\n",
            "\n",
            "--- Judge's Evaluation of Unsupported Answer ---\n",
            "**Step-by-step reasoning:**\n",
            "\n",
            "1.  **Analyze the Generated Answer's Claims:**\n",
            "    *   Claim 1: The primary reasons for the persistence of cVDPV outbreaks are low immunization coverage in certain areas and interruptions to vaccination campaigns.\n",
            "    *   Claim 2: The most prevalent type is cVDPV2.\n",
            "    *   Claim 3: A new, more resilient strain, cVDPV3, also emerged in late 2024 in South America, causing significant concern.\n",
            "\n",
            "2.  **Verify Each Claim Against the Source Documents:**\n",
            "    *   **Claim 1:** The source documents state that cVDPVs \"can emerge and cause paralysis in areas with low population poliovirus immunity.\" They also highlight the need to overcome \"barriers to reaching children\" and point to \"Delayed implementation of outbreak response campaigns and low-quality campaigns\" as reasons for spread. This claim is **supported**.\n",
            "    *   **Claim 2:** The source documents state that \"38 countries reported cVDPV type 2 (cVDPV2) outbreaks\" out of a total of 39 countries with outbreaks. It also specifies that \"In the 38 countries with cVDPV2 transmission, 70 distinct outbreaks were reported\" out of a total of 74 outbreaks. This clearly indicates cVDPV2 is the most prevalent type. This claim is **supported**.\n",
            "    *   **Claim 3:** The source documents only mention cVDPV type 1 and cVDPV type 2. There is no mention of \"cVDPV3\" anywhere in the text. Furthermore, the documents do not mention any outbreaks in \"South America,\" and the data period is from January 2023 to June 2024, so it cannot contain information about \"late 2024.\" This claim is **not supported** and is a complete fabrication.\n",
            "\n",
            "3.  **Conclusion:**\n",
            "    The generated answer is not faithful because it includes a significant piece of information (the emergence of cVDPV3 in South America in late 2024) that is entirely absent from the provided source documents. While the first two claims are supported, the inclusion of fabricated information makes the entire answer unfaithful.\n",
            "\n",
            "**Final Verdict: NOT SUPPORTED**\n"
          ]
        }
      ],
      "source": [
        "# We use the same setup as before, but with our new unsupported answer.\n",
        "\n",
        "# --- Inputs for the judge ---\n",
        "# The question and context remain the same from our previous RAG query.\n",
        "question = result[\"query\"]\n",
        "context_docs = \"\\n\\n\".join([doc.page_content for doc in result[\"source_documents\"]])\n",
        "\n",
        "# Here is our manually crafted unsupported answer\n",
        "unsupported_answer = \"The primary reasons for the persistence of cVDPV outbreaks are low immunization coverage in certain areas and interruptions to vaccination campaigns. The most prevalent type is cVDPV2. A new, more resilient strain, cVDPV3, also emerged in late 2024 in South America, causing significant concern.\"\n",
        "\n",
        "# --- Format the prompt for the judge ---\n",
        "judge_input = judge_prompt_template.format(\n",
        "    question=question,\n",
        "    context=context_docs,\n",
        "    answer=unsupported_answer\n",
        ")\n",
        "\n",
        "# --- Get the evaluation ---\n",
        "print(\"\\n--- ⚖️ Submitting MODIFIED Answer to LLM Judge ⚖️ ---\")\n",
        "evaluation = judge_llm(judge_input)\n",
        "\n",
        "# --- Print the judge's verdict ---\n",
        "print(\"\\n--- Judge's Evaluation of Unsupported Answer ---\")\n",
        "print(evaluation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFCKwb4MNaaI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
